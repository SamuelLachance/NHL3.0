{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf13jPsgp9N1EZ1hmznMCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelLachance/NHL3.0/blob/main/NHL_PRED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw5s1kEz_CH5",
        "outputId": "d7ffa91b-2f00-452a-84f4-4ae65c406a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_GF:  0.821567106283941\n",
            "Accuracy_GA:  0.8301008533747091\n",
            "MSE_GF:  0.1698555959652729\n",
            "RMSE_GF:  0.41213540974450724\n",
            "R-squared_GF:  0.941216647615658\n",
            "MSE_GA:  0.15201586958520805\n",
            "RMSE_GA:  0.38989212557476466\n",
            "R-squared_GA:  0.9479566836583305\n",
            "Carolina Hurricanes : [3.05500743] [50.02762128] New York Rangers : [3.05163397] [49.97237872] total: [6.1066414]\n",
            "Columbus Blue Jackets : [3.25029713] [47.05586871] Washington Capitals : [3.65701798] [52.94413129] total: [6.90731511]\n",
            "Florida Panthers : [3.47459924] [55.08613827] Philadelphia Flyers : [2.83297531] [44.91386173] total: [6.30757455]\n",
            "Minnesota Wild : [2.92264825] [46.07476406] New Jersey Devils : [3.42062514] [53.92523594] total: [6.34327339]\n",
            "Nashville Predators : [3.34296473] [50.9360065] Buffalo Sabres : [3.22010325] [49.0639935] total: [6.56306798]\n",
            "Ottawa Senators : [2.754292] [42.498421] Boston Bruins : [3.72663584] [57.501579] total: [6.48092784]\n",
            "Tampa Bay Lightning : [3.65532465] [54.58710962] Montreal Canadiens : [3.04099005] [45.41289038] total: [6.6963147]\n",
            "Toronto Maple Leafs : [2.98289137] [49.25522014] New York Islanders : [3.07309897] [50.74477986] total: [6.05599034]\n",
            "Arizona Coyotes : [3.14390066] [48.10717035] Winnipeg Jets : [3.39130113] [51.89282965] total: [6.53520179]\n",
            "Detroit Red Wings : [3.46989043] [50.75047939] St. Louis Blues : [3.36726751] [49.24952061] total: [6.83715794]\n",
            "Seattle Kraken : [3.21078482] [48.97093123] Dallas Stars : [3.34572685] [51.02906877] total: [6.55651167]\n",
            "Calgary Flames : [3.30787062] [51.10490628] Anaheim Ducks : [3.16483594] [48.89509372] total: [6.47270656]\n",
            "Vegas Golden Knights : [3.32716138] [50.05714499] Vancouver Canucks : [3.31956483] [49.94285501] total: [6.64672621]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from optparse import OptionContainer\n",
        "from textblob.en.np_extractors import filter_insignificant\n",
        "from http.cookiejar import LoadError\n",
        "import os\n",
        "import requests\n",
        "from textblob import TextBlob\n",
        "import datetime\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import warnings\n",
        "import xgboost as xgb\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import GammaRegressor\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#remove accents\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii\n",
        "\n",
        "def weird_division(x,y):\n",
        "    if y == 0:\n",
        "        return 0\n",
        "    return x / y\n",
        "\n",
        "#get the time\n",
        "yesterday = (datetime.date.today()- datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "today = datetime.date.today().strftime('%Y-%m-%d')\n",
        "tomorrow = (datetime.date.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "year = 20222023\n",
        "# Retrieve the list of game IDs for the season\n",
        "\n",
        "\n",
        "urlHist = 'https://www.naturalstattrick.com/games.php?fromseason=20182019&thruseason=20212022&stype=2&sit=all&loc=B&team=All&rate=n'\n",
        "req = requests.get(urlHist)\n",
        "\n",
        "soup = BeautifulSoup(req.content)\n",
        "\n",
        "top = list(soup.children)\n",
        "\n",
        "body = soup.find('body')\n",
        "\n",
        "body.find_all('th')\n",
        "\n",
        "columns = [item.text for item in body.find_all('th')]\n",
        "columns\n",
        "\n",
        "body.find_all('td')\n",
        "\n",
        "data = [e.text for e in body.find_all('td')]\n",
        "\n",
        "start = 0\n",
        "table= []\n",
        "\n",
        "while start+len(columns) <= len(data):\n",
        "    player = []\n",
        "    #use length of columns as iteration stop point to get list of info for 1 player \n",
        "    for i in range(start,start+len(columns)):\n",
        "        player.append(data[i])\n",
        "    #add player row to list\n",
        "    table.append(player)\n",
        "    #start at next player\n",
        "    start += len(columns)\n",
        "\n",
        "df = pd.DataFrame(table, columns = columns, dtype = 'float').set_index('')\n",
        "\n",
        "df.head()\n",
        "\n",
        "urlHist2 = 'https://www.naturalstattrick.com/teamtable.php?fromseason=20222023&thruseason=20222023&stype=2&sit=all&score=all&rate=y&team=all&loc=B&gpf=c&gp=10&fd=&td='\n",
        "req2 = requests.get(urlHist2)\n",
        "\n",
        "soup2 = BeautifulSoup(req2.content)\n",
        "\n",
        "top2 = list(soup2.children)\n",
        "\n",
        "body2 = soup2.find('body')\n",
        "\n",
        "body2.find_all('th')\n",
        "\n",
        "columns2 = [item.text for item in body2.find_all('th')]\n",
        "columns2\n",
        "\n",
        "body2.find_all('td')\n",
        "\n",
        "data2 = [e.text for e in body2.find_all('td')]\n",
        "\n",
        "start2 = 0\n",
        "table2 = []\n",
        "\n",
        "while start2+len(columns2) <= len(data2):\n",
        "    player = []\n",
        "    #use length of columns as iteration stop point to get list of info for 1 player \n",
        "    for i in range(start2,start2+len(columns2)):\n",
        "        player.append(data2[i])\n",
        "    #add player row to list\n",
        "    table2.append(player)\n",
        "    #start at next player\n",
        "    start2 += len(columns2)\n",
        "\n",
        "df2 = pd.DataFrame(table2, columns = columns2, dtype = 'float').set_index('')\n",
        "\n",
        "df2.head()\n",
        "\n",
        "urlHist3 = 'https://www.naturalstattrick.com/teamtable.php?fromseason=20222023&thruseason=20222023&stype=2&sit=all&score=all&rate=y&team=all&loc=B&gpf=410&fd=&td='\n",
        "req3 = requests.get(urlHist3)\n",
        "\n",
        "soup3 = BeautifulSoup(req3.content)\n",
        "\n",
        "top3 = list(soup3.children)\n",
        "\n",
        "body3 = soup3.find('body')\n",
        "\n",
        "body3.find_all('th')\n",
        "\n",
        "columns3 = [item.text for item in body3.find_all('th')]\n",
        "columns3\n",
        "\n",
        "body3.find_all('td')\n",
        "\n",
        "data3 = [e.text for e in body3.find_all('td')]\n",
        "\n",
        "start3 = 0\n",
        "table3 = []\n",
        "\n",
        "while start3+len(columns3) <= len(data3):\n",
        "    player = []\n",
        "    #use length of columns as iteration stop point to get list of info for 1 player \n",
        "    for i in range(start3,start3+len(columns3)):\n",
        "        player.append(data3[i])\n",
        "    #add player row to list\n",
        "    table3.append(player)\n",
        "    #start at next player\n",
        "    start3 += len(columns3)\n",
        "\n",
        "df3 = pd.DataFrame(table3, columns = columns3, dtype = 'float').set_index('')\n",
        "\n",
        "df3.head()\n",
        "\n",
        "\n",
        "# Diviser les données en données d'entraînement et de test\n",
        "X = df[['xGF', 'xGA','HDCF', 'HDCA','MDCF','MDCA','HDSF','HDSA','MDSF','MDSA','SH%','SV%']]  # Les variables prédictives\n",
        "y_gf = df['GF']  # La variable cible \"GF\"\n",
        "y_ga = df['GA']  # La variable cible \"GA\"\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_gf_train, y_gf_test, y_ga_train, y_ga_test = train_test_split(X, y_gf, y_ga, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': None,\n",
        "    'min_samples_split': 5\n",
        "}\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 1000,\n",
        "    'gamma': 0\n",
        "}\n",
        "\n",
        "param_grid_gbm = {\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 1000,\n",
        "    'max_depth': 3,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2\n",
        "}\n",
        "# Train the random forest model on the training data for \"GF\"\n",
        "rf_gf = RandomForestRegressor(**param_grid_rf)\n",
        "rf_gf.fit(X_train, y_gf_train)\n",
        "\n",
        "# Train the random forest model on the training data for \"GA\"\n",
        "rf_ga = RandomForestRegressor(**param_grid_rf)\n",
        "rf_ga.fit(X_train, y_ga_train)\n",
        "\n",
        "# Train the XGBoost model on the training data for \"GF\"\n",
        "xgb_gf = XGBRegressor(**param_grid_xgb)\n",
        "xgb_gf.fit(X_train, y_gf_train)\n",
        "\n",
        "# Train the XGBoost model on the training data for \"GA\"\n",
        "xgb_ga = XGBRegressor(**param_grid_xgb)\n",
        "xgb_ga.fit(X_train, y_ga_train)\n",
        "\n",
        "# Train the gradient boosting model on the training data for \"GF\"\n",
        "gbm_gf = GradientBoostingRegressor(**param_grid_gbm)\n",
        "gbm_gf.fit(X_train, y_gf_train)\n",
        "\n",
        "# Train the gradient boosting model on the training data for \"GA\"\n",
        "gbm_ga = GradientBoostingRegressor(**param_grid_gbm)\n",
        "gbm_ga.fit(X_train, y_ga_train)\n",
        "\n",
        "# Create an instance of SVR model for \"GF\"\n",
        "svr_gf = SVR(kernel='rbf', C=1e3, gamma='scale')\n",
        "svr_gf.fit(X_train, y_gf_train)\n",
        "\n",
        "\n",
        "# Create an instance of SVR model for \"GA\"\n",
        "svr_ga = SVR(kernel='rbf', C=1e3, gamma='scale')\n",
        "svr_ga.fit(X_train, y_ga_train)\n",
        "\n",
        "def find_best_W(X, y, models, weights=None, n_iterations=1000):\n",
        "    \n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train each model on the training set\n",
        "    preds_train = []\n",
        "    for model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "        preds_train.append(model.predict(X_train))\n",
        "    \n",
        "    # If no weights are provided, initialize them to equal values\n",
        "    if weights is None:\n",
        "        weights = np.full(len(models), 1/len(models))\n",
        "    \n",
        "    # Define the objective function as the validation set performance\n",
        "    def objective_function(weights):\n",
        "        preds_val = [model.predict(X_val) for model in models]\n",
        "        combined_preds = np.average(preds_val, axis=0, weights=weights)\n",
        "        return mean_squared_error(y_val, combined_preds)\n",
        "    \n",
        "    # Run the optimization algorithm\n",
        "    best_weights = weights\n",
        "    best_score = objective_function(weights)\n",
        "    for i in range(n_iterations):\n",
        "        new_weights = [w + random.uniform(-0.1, 0.1) for w in weights]  # perturb the weights\n",
        "        new_weights = np.clip(new_weights, 0, 1)  # ensure weights are between 0 and 1\n",
        "        score = objective_function(new_weights)\n",
        "        if score < best_score:\n",
        "            best_weights = new_weights\n",
        "            best_score = score\n",
        "        weights = new_weights\n",
        "    \n",
        "    # Normalize the weights\n",
        "    weights_sum = sum(best_weights)\n",
        "    best_weights = [w/weights_sum for w in best_weights]\n",
        "    \n",
        "    # Create a dictionary of model weights\n",
        "    model_weights = {}\n",
        "    for i, model in enumerate(models):\n",
        "        model_weights[type(model).__name__] = best_weights[i]\n",
        "    \n",
        "    return model_weights\n",
        "\n",
        "#best_weights = find_best_W(X, y_gf, [xgb_gf, gbm_gf, svr_gf], None, 1000)\n",
        "\n",
        "#print(best_weights)\n",
        "\n",
        "# Combine the models\n",
        "def Test_model_GF(X):\n",
        "    \n",
        "    # Use the trained models to make predictions for \"GF\"\n",
        "    rf_gf_pred = rf_gf.predict(X)\n",
        "    xgb_gf_pred = xgb_gf.predict(X)\n",
        "    gbm_gf_pred = gbm_gf.predict(X)\n",
        "    svr_gf_pred = svr_gf.predict(X)\n",
        "    \n",
        "    # Combine the predictions using a weighted average\n",
        "    gf_pred = (0.09264992620841171*xgb_gf_pred) + (0.6658086047607201*gbm_gf_pred)  + (0.24154146903086823*svr_gf_pred) #(rf_gf_pred + xgb_gf_pred + svr_gf_pred)/3 #(0.22221016502179602*rf_gf_pred) + (0.250744771234347*xgb_gf_pred) + (0.20498370405959596*gbm_gf_pred) + (0.3220613596842609*svr_gf_pred)\n",
        "    \n",
        "    return gf_pred\n",
        "\n",
        "def Test_model_GA(X):\n",
        "    # Use the trained models to make predictions\n",
        "    # Use the trained models to make predictions for \"GA\"\n",
        "    rf_ga_pred = rf_ga.predict(X)\n",
        "    xgb_ga_pred = xgb_ga.predict(X)\n",
        "    gbm_ga_pred = gbm_ga.predict(X)\n",
        "    svr_ga_pred = svr_ga.predict(X)\n",
        "    \n",
        "    # Combine the predictions using a weighted average\n",
        "    ga_pred = (0.6097886212917437*xgb_ga_pred) + (0.3902113787082563*svr_ga_pred) #(rf_ga_pred + xgb_ga_pred + svr_ga_pred)/3 #(0.27043590130033*rf_ga_pred) + (0.2693404847288102*xgb_ga_pred) + (0.18025115844990966*gbm_ga_pred) + (0.27997245552095007*svr_ga_pred)\n",
        "    \n",
        "    return ga_pred\n",
        "\n",
        "def combined_model_goals(home_xGF,home_xGA,home_HDCF, home_HDCA, home_MDCF, home_MDCA,home_HDSF,home_HDSA,home_MDSF,home_MDSA,home_SH,home_SV):\n",
        "    # Convert the input arguments into a numpy array\n",
        "    X = np.array([[home_xGF,home_xGA,home_HDCF, home_HDCA, home_MDCF, home_MDCA,home_HDSF,home_HDSA,home_MDSF,home_MDSA,home_SH,home_SV]])\n",
        "    # Use the trained models to make predictions for \"GF\"\n",
        "    rf_gf_pred = rf_gf.predict(X)\n",
        "    xgb_gf_pred = xgb_gf.predict(X)\n",
        "    gbm_gf_pred = gbm_gf.predict(X)\n",
        "    svr_gf_pred = svr_gf.predict(X)\n",
        "    \n",
        "    # Use the trained models to make predictions for \"GA\"\n",
        "    rf_ga_pred = rf_ga.predict(X)\n",
        "    xgb_ga_pred = xgb_ga.predict(X)\n",
        "    gbm_ga_pred = gbm_ga.predict(X)\n",
        "    svr_ga_pred = svr_ga.predict(X)\n",
        "    \n",
        "    # Combine the predictions using a weighted average\n",
        "    gf_pred = (0.09264992620841171*xgb_gf_pred) + (0.6658086047607201*gbm_gf_pred)  + (0.24154146903086823*svr_gf_pred) #(0.22221016502179602*rf_gf_pred) + (0.250744771234347*xgb_gf_pred) + (0.20498370405959596*gbm_gf_pred) + (0.3220613596842609*svr_gf_pred)\n",
        "    ga_pred = (0.6097886212917437*xgb_ga_pred) + (0.3902113787082563*svr_ga_pred) #(0.27043590130033*rf_ga_pred) + (0.2693404847288102*xgb_ga_pred) + (0.18025115844990966*gbm_ga_pred) + (0.27997245552095007*svr_ga_pred)\n",
        "    \n",
        "    return gf_pred, ga_pred\n",
        "\n",
        "y_pred_combined_GF = Test_model_GF(X_test)\n",
        "y_pred_combined_GA = Test_model_GA(X_test)\n",
        "accuracy_GF = accuracy_score(y_gf_test, np.round(y_pred_combined_GF))\n",
        "accuracy_GA = accuracy_score(y_ga_test, np.round(y_pred_combined_GA))\n",
        "print(\"Accuracy_GF: \", accuracy_GF)\n",
        "print(\"Accuracy_GA: \", accuracy_GA)\n",
        "\n",
        "mse_GF = mean_squared_error(y_gf_test, y_pred_combined_GF)\n",
        "rmse_GF = np.sqrt(mse_GF)\n",
        "r2_GF = r2_score(y_gf_test, y_pred_combined_GF)\n",
        "\n",
        "mse_GA = mean_squared_error(y_ga_test, y_pred_combined_GA)\n",
        "rmse_GA = np.sqrt(mse_GA)\n",
        "r2_GA = r2_score(y_ga_test, y_pred_combined_GA)\n",
        "\n",
        "print(\"MSE_GF: \", mse_GF)\n",
        "print(\"RMSE_GF: \", rmse_GF)\n",
        "print(\"R-squared_GF: \", r2_GF)\n",
        "\n",
        "print(\"MSE_GA: \", mse_GA)\n",
        "print(\"RMSE_GA: \", rmse_GA)\n",
        "print(\"R-squared_GA: \", r2_GA)\n",
        "\n",
        "\n",
        "# Récupérer les matchs à venir à partir de l'API NHL\n",
        "response = requests.get(\"https://statsapi.web.nhl.com/api/v1/schedule?date=\" + today + \"&expand=schedule.linescore\")\n",
        "schedule = response.json()['dates'][0]['games']\n",
        "\n",
        "# Prédire les résultats de chaque match\n",
        "for game in schedule:\n",
        "    home_team = remove_accents(game['teams']['home']['team']['name']).decode()\n",
        "    away_team = remove_accents(game['teams']['away']['team']['name']).decode()\n",
        "    team_df_H = df2.loc[df2['Team'] == home_team]\n",
        "    team_df_A = df2.loc[df2['Team'] == away_team]\n",
        "    team_df_H2 = df3.loc[df3['Team'] == home_team]\n",
        "    team_df_A2 = df3.loc[df3['Team'] == away_team]\n",
        "\n",
        "    try:\n",
        "      home_team.index('Canadiens')\n",
        "    except ValueError:\n",
        "      try:\n",
        "        home_team.index('Blues')\n",
        "      except ValueError:\n",
        "        team_df_H2 = df3.loc[df3['Team'] == home_team]\n",
        "      else:\n",
        "        team_df_H2 = df3.loc[df3['Team'] == 'St Louis Blues']\n",
        "    else:\n",
        "      team_df_H2 = df3.loc[df3['Team'] == 'Montreal Canadiens']\n",
        "      \n",
        "    try:\n",
        "        away_team.index('Canadiens')\n",
        "    except ValueError:\n",
        "        try:\n",
        "            away_team.index('Blues')\n",
        "        except ValueError:\n",
        "            team_df_A2 = df3.loc[df3['Team'] == away_team]\n",
        "        else:\n",
        "            team_df_A2 = df3.loc[df3['Team'] == 'St Louis Blues']\n",
        "    else:\n",
        "        team_df_A2 = df3.loc[df3['Team'] == 'Montreal Canadiens']\n",
        "        \n",
        "    try:\n",
        "        # Calculate the values of the predictor variables for each team\n",
        "        home_SH = team_df_H2['SH%'].values[0]\n",
        "        home_SV = team_df_H2['SV%'].values[0]\n",
        "        away_SH = team_df_A2['SH%'].values[0]\n",
        "        away_SV = team_df_A2['SV%'].values[0]\n",
        "    except IndexError:\n",
        "        print('Data not found for one of the teams')\n",
        "\n",
        "    try:\n",
        "      home_team.index('Canadiens')\n",
        "    except ValueError:\n",
        "      try:\n",
        "        home_team.index('Blues')\n",
        "      except ValueError:\n",
        "        team_df_H = df2.loc[df2['Team'] == home_team]\n",
        "      else:\n",
        "        team_df_H = df2.loc[df2['Team'] == 'St Louis Blues']\n",
        "    else:\n",
        "      team_df_H = df2.loc[df2['Team'] == 'Montreal Canadiens']\n",
        "      \n",
        "    try:\n",
        "        away_team.index('Canadiens')\n",
        "    except ValueError:\n",
        "        try:\n",
        "            away_team.index('Blues')\n",
        "        except ValueError:\n",
        "            team_df_A = df2.loc[df2['Team'] == away_team]\n",
        "        else:\n",
        "            team_df_A = df2.loc[df2['Team'] == 'St Louis Blues']\n",
        "    else:\n",
        "        team_df_A = df2.loc[df2['Team'] == 'Montreal Canadiens']\n",
        "        \n",
        "    try:\n",
        "        # Calculate the values of the predictor variables for each team\n",
        "        home_xGF = team_df_H['xGF/60'].values[0]\n",
        "        home_xGA = team_df_H['xGA/60'].values[0]\n",
        "        home_HDCF = team_df_H['HDCF/60'].values[0]\n",
        "        home_HDCA = team_df_H['HDCA/60'].values[0]\n",
        "        home_MDCF = team_df_H['MDCF/60'].values[0]\n",
        "        home_MDCA = team_df_H['MDCA/60'].values[0]\n",
        "        home_HDSF = team_df_H['HDSF/60'].values[0]\n",
        "        home_HDSA = team_df_H['HDSA/60'].values[0]\n",
        "        home_MDSF = team_df_H['MDSF/60'].values[0]\n",
        "        home_MDSA = team_df_H['MDSA/60'].values[0]\n",
        "        \n",
        "        away_xGF = team_df_A['xGF/60'].values[0]\n",
        "        away_xGA = team_df_A['xGA/60'].values[0]\n",
        "        away_HDCF = team_df_A['HDCF/60'].values[0]\n",
        "        away_HDCA = team_df_A['HDCA/60'].values[0]\n",
        "        away_MDCF = team_df_A['MDCF/60'].values[0]\n",
        "        away_MDCA = team_df_A['MDCA/60'].values[0]\n",
        "        away_HDSF = team_df_A['HDSF/60'].values[0]\n",
        "        away_HDSA = team_df_A['HDSA/60'].values[0]\n",
        "        away_MDSF = team_df_A['MDSF/60'].values[0]\n",
        "        away_MDSA = team_df_A['MDSA/60'].values[0]\n",
        "\n",
        "\n",
        "    except IndexError:\n",
        "        print('Data not found for one of the teams')\n",
        "    # Predict the outcome of the game using the logistic regression model\n",
        "    #prediction = combined_model_W( home_xGF, home_xGA, home_HDCF ,home_HDCA, home_MDCF, home_MDCA, home_PDO)\n",
        "    H_GF,H_GA = combined_model_goals(home_xGF,home_xGA,home_HDCF, home_HDCA, home_MDCF, home_MDCA,home_HDSF,home_HDSA,home_MDSF,home_MDSA,home_SH,home_SV)\n",
        "    A_GF,A_GA = combined_model_goals(away_xGF,away_xGA,away_HDCF, away_HDCA, away_MDCF, away_MDCA,away_HDSF,away_HDSA,away_MDSF,away_MDSA,away_SH,away_SV)\n",
        "    score_H = (H_GF + A_GA)/2\n",
        "    score_A = (A_GF + H_GA)/2\n",
        "    total = score_H + score_A\n",
        "    imp_odds_H = (score_H / total)*100\n",
        "    imp_odds_A = (score_A / total)*100\n",
        "\n",
        "    print(away_team, ':', score_A, imp_odds_A, home_team, ':', score_H, imp_odds_H, 'total:', total)"
      ]
    }
  ]
}